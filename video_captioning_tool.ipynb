{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# üîÅ Install dependencies, upload video, extract frames, caption, and summarize\n",
        "!pip install transformers torch torchvision accelerate opencv-python Pillow --quiet\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from google.colab import files\n",
        "\n",
        "# üîº Upload video\n",
        "print(\"üì§ Please upload your video file (.mp4, .mov)...\")\n",
        "uploaded = files.upload()\n",
        "video_path = next((f for f in uploaded if f.endswith(('.mp4', '.mov', '.avi'))), None)\n",
        "if not video_path:\n",
        "    raise ValueError(\"‚ùå No supported video file uploaded.\")\n",
        "\n",
        "# üß† Setup device and model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "# üéûÔ∏è Extract keyframes\n",
        "def extract_keyframes(video_path, interval=2):\n",
        "    os.makedirs(\"frames\", exist_ok=True)\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    success, image = vidcap.read()\n",
        "    count, sec = 0, 0\n",
        "\n",
        "    while success:\n",
        "        if int(sec) % interval == 0:\n",
        "            frame_path = f\"frames/frame{count}.jpg\"\n",
        "            cv2.imwrite(frame_path, image)\n",
        "            count += 1\n",
        "        sec += 1\n",
        "        vidcap.set(cv2.CAP_PROP_POS_MSEC, sec * 1000)\n",
        "        success, image = vidcap.read()\n",
        "    print(f\"‚úÖ Extracted {count} keyframes.\")\n",
        "\n",
        "# üßæ Caption image\n",
        "def caption_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
        "    out = model.generate(**inputs)\n",
        "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# üßπ Clean and merge captions\n",
        "def deduplicate_and_merge(captions, threshold=1):\n",
        "    freq = Counter(captions)\n",
        "    unique_captions = [cap for cap, count in freq.items() if count > threshold]\n",
        "    if not unique_captions:\n",
        "        unique_captions = list(set(captions))\n",
        "    return \". \".join(unique_captions).capitalize() + \".\"\n",
        "\n",
        "# üß† Main execution\n",
        "def generate_video_description(video_path):\n",
        "    extract_keyframes(video_path, interval=2)\n",
        "    frame_files = sorted([f\"frames/{f}\" for f in os.listdir(\"frames\") if f.endswith(\".jpg\")])\n",
        "    captions = []\n",
        "\n",
        "    print(\"\\nüîç Generating captions for frames...\\n\")\n",
        "    for frame in frame_files:\n",
        "        cap = caption_image(frame)\n",
        "        print(f\"{frame} ‚ûú {cap}\")\n",
        "        captions.append(cap)\n",
        "\n",
        "    description = deduplicate_and_merge(captions)\n",
        "    print(\"\\nüé¨ Final Video Description:\\n\")\n",
        "    print(description)\n",
        "\n",
        "generate_video_description(video_path)"
      ],
      "metadata": {
        "id": "aKisYbOvPsB4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}